{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import subprocess\n",
    "import torch\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to read and write text/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream_size(stream):\n",
    "    result = sum(1 for _ in stream)\n",
    "    stream.seek(0)\n",
    "    return result\n",
    "\n",
    "def read_token_file(file_name: str):\n",
    "    out = []\n",
    "    with open(file_name, 'r') as stream:\n",
    "            file_size = get_stream_size(stream)\n",
    "            for line in stream:\n",
    "                tokens = line.strip().split()\n",
    "                out.append(tokens)\n",
    "    return out\n",
    "\n",
    "def read_text_file(file_name: str):\n",
    "    out = []\n",
    "    with open(file_name, 'r') as stream:\n",
    "            file_size = get_stream_size(stream)\n",
    "            for line in stream:\n",
    "                tokens = line.strip()\n",
    "                out.append(tokens)\n",
    "    return out\n",
    "\n",
    "def write_text_from_tokens(tokens, output_file):\n",
    "    with open(output_file, 'w+') as out_stream:\n",
    "        for token in tokens:\n",
    "            out_stream.write(' '.join(token) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Information Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class corpus_information():\n",
    "    def __init__(self, corpus, language, dataset_name, n_most_common=20, remove_punctuation=False):\n",
    "        self.corpus = corpus\n",
    "        self.language = language\n",
    "        self.dataset_name = dataset_name\n",
    "        self.n_most_common = n_most_common\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.counter = self._counter_corpus()\n",
    "        self.count_words = self._count_words()\n",
    "        self.count_unique_words = self._count_unique_words()\n",
    "        self.most_common_words = self._most_common_words()\n",
    "        self.count_sequences = self._count_sequences()\n",
    "        self.max_sequences_length = self._max_sequences_length()\n",
    "        self.mean_sequences_length = self._mean_sequences_length()\n",
    "\n",
    "    def _counter_corpus(self):\n",
    "        reg = r\"[\\w']+|[.,!?;:'()\\[\\]{}\\\"]\"\n",
    "        if self.remove_punctuation:\n",
    "            reg = r'\\w+'\n",
    "        return collections.Counter([word for sentence in self.corpus for word in re.findall(reg, sentence)])\n",
    "    \n",
    "    def _count_words(self):\n",
    "        reg = r\"[\\w']+|[.,!?;:'()\\[\\]{}\\\"]\"\n",
    "        if self.remove_punctuation:\n",
    "            reg = r'\\w+'\n",
    "        return len([word for sentence in self.corpus for word in re.findall(reg, sentence)])\n",
    "\n",
    "    def _count_unique_words(self):\n",
    "        return len(self.counter)\n",
    "    \n",
    "    def _most_common_words(self):\n",
    "        return list(zip(*self.counter.most_common(self.n_most_common)))[0]\n",
    "    \n",
    "    def _count_sequences(self):\n",
    "        return len(self.corpus)\n",
    "    \n",
    "    def _max_sequences_length(self):\n",
    "        return np.max([len(sentence) for sentence in self.corpus])\n",
    "    \n",
    "    def _mean_sequences_length(self):\n",
    "        return np.mean([len(sentence) for sentence in self.corpus])\n",
    "    \n",
    "    def show_informations(self):\n",
    "        print(self.dataset_name+': ')\n",
    "        print(f'{self.count_words} {self.language} words.')\n",
    "        print(f'{self.count_unique_words} unique {self.language} words.')\n",
    "        print(f'{self.n_most_common} Most common words in the {self.dataset_name} :')\n",
    "        print('\"' + '\" \"'.join(self.most_common_words) + '\"')\n",
    "        print(f'{self.count_sequences} sequences in {self.dataset_name}')\n",
    "        print(f'The longest sequence as a length of {self.max_sequences_length}.')\n",
    "        print('The mean sequence length is {:.2f}.'.format(self.mean_sequences_length))\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized alligned texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_token_file = read_token_file('../data/train.lang1')\n",
    "fr_token_file = read_token_file('../data/train.lang2')\n",
    "en_aligned_text_file = [' '.join(word) for word in en_token_file]\n",
    "fr_aligned_text_file = [' '.join(word) for word in fr_token_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligned texts information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned English DataSet: \n",
      "203498 English words.\n",
      "13530 unique English words.\n",
      "20 Most common words in the Aligned English DataSet :\n",
      "\"the\" \"of\" \"to\" \"and\" \"in\" \"a\" \"is\" \"that\" \"i\" \"it\" \"we\" \"this\" \"for\" \"on\" \"be\" \"are\" \"not\" \"have\" \"you\" \"with\"\n",
      "11000 sequences in Aligned English DataSet\n",
      "The longest sequence as a length of 512.\n",
      "The mean sequence length is 105.17.\n",
      "\n",
      "\n",
      "Aligned French DataSet: \n",
      "249567 French words.\n",
      "17988 unique French words.\n",
      "20 Most common words in the Aligned French DataSet :\n",
      "\".\" \"de\" \",\" \"la\" \"et\" \"le\" \"à\" \"les\" \"des\" \"l'\" \"que\" \"est\" \"en\" \"d'\" \"un\" \"une\" \"du\" \"pour\" \"a\" \"qui\"\n",
      "11000 sequences in Aligned French DataSet\n",
      "The longest sequence as a length of 562.\n",
      "The mean sequence length is 123.90.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_aligned_info = corpus_information(en_aligned_text_file, 'English', 'Aligned English DataSet')\n",
    "fr_aligned_info = corpus_information(fr_aligned_text_file, 'French', 'Aligned French DataSet')\n",
    "en_aligned_info.show_informations()\n",
    "fr_aligned_info.show_informations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not tokenized and not alligned texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text_file = read_text_file('../data/unaligned.en')\n",
    "fr_text_file = read_text_file('../data/unaligned.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaligned English DataSet: \n",
      "9648962 English words.\n",
      "74560 unique English words.\n",
      "20 Most common words in the Unaligned English DataSet :\n",
      "\"the\" \".\" \",\" \"to\" \"of\" \"and\" \"in\" \"a\" \"is\" \"that\" \"I\" \"for\" \"this\" \"be\" \"on\" \"we\" \"it\" \"are\" \"have\" \"not\"\n",
      "474000 sequences in Unaligned English DataSet\n",
      "The longest sequence as a length of 788.\n",
      "The mean sequence length is 106.46.\n",
      "\n",
      "\n",
      "Unaligned French DataSet: \n",
      "10244614 French words.\n",
      "97985 unique French words.\n",
      "20 Most common words in the Unaligned French DataSet :\n",
      "\".\" \"de\" \",\" \"la\" \"et\" \"le\" \"à\" \"les\" \"des\" \"que\" \"en\" \"est\" \"du\" \"un\" \"une\" \"pour\" \"nous\" \"dans\" \"pas\" \"qui\"\n",
      "474000 sequences in Unaligned French DataSet\n",
      "The longest sequence as a length of 969.\n",
      "The mean sequence length is 119.91.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_unaligned_info = corpus_information(en_text_file, 'English', 'Unaligned English DataSet')\n",
    "fr_unaligned_info = corpus_information(fr_text_file, 'French', 'Unaligned French DataSet')\n",
    "en_unaligned_info.show_informations()\n",
    "fr_unaligned_info.show_informations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
