{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First look at the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to read and write text/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream_size(stream):\n",
    "    result = sum(1 for _ in stream)\n",
    "    stream.seek(0)\n",
    "    return result\n",
    "\n",
    "def read_token_file(file_name: str):\n",
    "    out = []\n",
    "    with open(file_name, 'r') as stream:\n",
    "            file_size = get_stream_size(stream)\n",
    "            for line in stream:\n",
    "                tokens = line.strip().split()\n",
    "                out.append(tokens)\n",
    "    return out\n",
    "\n",
    "def read_text_file(file_name: str):\n",
    "    out = []\n",
    "    with open(file_name, 'r') as stream:\n",
    "            file_size = get_stream_size(stream)\n",
    "            for line in stream:\n",
    "                tokens = line.strip()\n",
    "                out.append(tokens)\n",
    "    return out\n",
    "\n",
    "def write_text_from_tokens(tokens, output_file):\n",
    "    with open(output_file, 'w+') as out_stream:\n",
    "        for token in tokens:\n",
    "            out_stream.write(' '.join(token) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized alligned texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_token_file = read_token_file('data/train.lang1')\n",
    "fr_token_file = read_token_file('data/train.lang2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not tokenized and not alligned texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text_file = read_text_file('data/unaligned.en')\n",
    "fr_text_file = read_text_file('data/unaligned.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write tokens to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_from_tokens(fr_token_file, 'french.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compute bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(pred_file_path: str, target_file_path: str, print_all_scores: bool):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_file_path: the file path that contains the predictions.\n",
    "        target_file_path: the file path that contains the targets (also called references).\n",
    "        print_all_scores: if True, will print one score per example.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    out = subprocess.run([\"sacrebleu\", \"--input\", pred_file_path, target_file_path, '--tokenize',\n",
    "                          'none', '--sentence-level', '--score-only'],\n",
    "                         stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    lines = out.stdout.split('\\n')\n",
    "    #if print_all_scores:\n",
    "        #print('\\n'.join(lines[:-1]))\n",
    "    \n",
    "    scores = [float(x) for x in lines[:-1]]\n",
    "    #print('final avg bleu score: {:.2f}'.format(sum(scores) / len(scores)))\n",
    "    \n",
    "    return scores, sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bleu French vs. French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_score, score = compute_bleu('data/train.lang2', 'data/train.lang2', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bleu French vs. English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_score, score = compute_bleu('data/train.lang2', 'data/train.lang1', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7819545454545425"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang(Enum):\n",
    "    \"\"\"Feature which the dataloader can load.\"\"\"\n",
    "\n",
    "    french = 'french'\n",
    "    english = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
